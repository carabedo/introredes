{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b76351",
   "metadata": {},
   "source": [
    "# la neurona\n",
    "\n",
    "![](https://c.tenor.com/K9q23o-X8HEAAAAC/brain-neurons.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1051925",
   "metadata": {},
   "source": [
    "# \"la neurona\"\n",
    "\n",
    "![](https://i.stack.imgur.com/Ql0YP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7e493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la neurona, recibe de input un vector (x1,x2,...,xn)\n",
    "# usa los pesos W (w1,w2,...,wn)\n",
    "import numpy as np\n",
    "\n",
    "def f(x, w):\n",
    "    \n",
    "    # suma pesada\n",
    "    # sumo cada par xn*wn y al final el bias \n",
    "    # eso es el producto interno o dot product en algebra\n",
    "    wsum = np.dot(x,w[1:]) + w[0]\n",
    "    \n",
    "    # activacion, si la suma pesada supera cierto limite la funcion\n",
    "    # tira un 1\n",
    "    # si no, tira un 0\n",
    "    \n",
    "    if wsum >= 0.0:\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3e9061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.781084</td>\n",
       "      <td>2.550537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.465489</td>\n",
       "      <td>2.362125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.396562</td>\n",
       "      <td>4.400294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.388070</td>\n",
       "      <td>1.850220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.064072</td>\n",
       "      <td>3.005306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.627531</td>\n",
       "      <td>2.759262</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.332441</td>\n",
       "      <td>2.088627</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.922597</td>\n",
       "      <td>1.771064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.675419</td>\n",
       "      <td>-0.242069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.673756</td>\n",
       "      <td>3.508563</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  y\n",
       "0  2.781084  2.550537  0\n",
       "1  1.465489  2.362125  0\n",
       "2  3.396562  4.400294  0\n",
       "3  1.388070  1.850220  0\n",
       "4  3.064072  3.005306  0\n",
       "5  7.627531  2.759262  1\n",
       "6  5.332441  2.088627  1\n",
       "7  6.922597  1.771064  1\n",
       "8  8.675419 -0.242069  1\n",
       "9  7.673756  3.508563  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "\n",
    "data=pd.DataFrame(dataset,columns=['x1','x2','y'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a526a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[['x1','x2']]\n",
    "y=data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0aaa46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n"
     ]
    }
   ],
   "source": [
    "# veamos como funciona si los pesos ya nos vienen definidos\n",
    "# un peso por cada xn y un bias (3) \n",
    "\n",
    "weights = np.array([-0.1, 0.20653640140000007, -0.23418117710000003])\n",
    "\n",
    "# miro el output de mi neurona para cada fila\n",
    "# comparo con la columna 'y' el target\n",
    "\n",
    "for i,x in enumerate(X.values):\n",
    "    prediction = f(x, weights)\n",
    "    print(\"Target=%d, Prediccion=%d\" % (y[i], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892d23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=1\n",
      "Target=0, Prediccion=1\n",
      "Target=0, Prediccion=1\n",
      "Target=0, Prediccion=0\n",
      "Target=1, Prediccion=0\n",
      "Target=1, Prediccion=0\n",
      "Target=1, Prediccion=0\n",
      "Target=1, Prediccion=0\n",
      "Target=1, Prediccion=0\n"
     ]
    }
   ],
   "source": [
    "# veamos como funciona si los pesos ya nos vienen definidos\n",
    "# un peso por cada xn y un bias (3) \n",
    "\n",
    "weights = np.array([5, -91, 91])\n",
    "\n",
    "# miro el output de mi neurona para cada fila\n",
    "# comparo con la columna 'y' el target\n",
    "\n",
    "for i,x in enumerate(X.values):\n",
    "    prediction = f(x, weights)\n",
    "    print(\"Target=%d, Prediccion=%d\" % (y[i], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a0db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pero como obtengo los parametros/pesos y biases? \n",
    "# Como obtenia los betas/parametros de cualquier modelo?\n",
    "\n",
    "\n",
    "# descenso gradiente!\n",
    "\n",
    "def train_weights(x_train,y_train, l_rate, n_epoch):\n",
    "    #inicio con w = 0,0...0\n",
    "    weights = [0.0 for i in range(len(x_train[0])+1)]\n",
    "    \n",
    "    print('>epoch=-1, lrate=%.3f, error=inf,w0=%.3f, w1=%.3f, w2=%.3f' %(l_rate,weights[0], weights[1], weights[2])) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #itero\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        \n",
    "        #predigo sobre las filas\n",
    "        for i,x in enumerate(x_train):\n",
    "            prediction = f(x, weights)\n",
    "            \n",
    "            # calculo la funcion de costo\n",
    "            error = y_train[i] - prediction\n",
    "            sum_error += error**2\n",
    "            \n",
    "            # defino los nuevos pesos en la direccion del gradiente\n",
    "            # l_rate era la longuitud del paso en esa direccion\n",
    "            \n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(x)):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * x[i]\n",
    "                \n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f, w0=%.3f, w1=%.3f, w2=%.3f' %\n",
    "              (epoch, l_rate, sum_error, weights[0], weights[1], weights[2]))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ba7653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=-1, lrate=0.100, error=inf,w0=0.000, w1=0.000, w2=0.000\n",
      ">epoch=0, lrate=0.100, error=2.000, w0=0.000, w1=0.485, w2=0.021\n",
      ">epoch=1, lrate=0.100, error=1.000, w0=-0.100, w1=0.207, w2=-0.234\n",
      ">epoch=2, lrate=0.100, error=0.000, w0=-0.100, w1=0.207, w2=-0.234\n",
      ">epoch=3, lrate=0.100, error=0.000, w0=-0.100, w1=0.207, w2=-0.234\n",
      ">epoch=4, lrate=0.100, error=0.000, w0=-0.100, w1=0.207, w2=-0.234\n"
     ]
    }
   ],
   "source": [
    "# ahora si, entrenemos nuestra neurona\n",
    "\n",
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "\n",
    "weights = train_weights(X.values,y, l_rate, n_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014baedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=0, Prediccion=0\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n",
      "Target=1, Prediccion=1\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(X.values):\n",
    "    prediction = f(x, weights)\n",
    "    print(\"Target=%d, Prediccion=%d\" % (y[i], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09ff6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1, 0.20653640140000007, -0.23418117710000003]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354a05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7931557",
   "metadata": {},
   "source": [
    "# No es una regresion logistica con sombrero nuevo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d4270",
   "metadata": {},
   "source": [
    "# \"la neurona\"\n",
    "\n",
    "![](https://i.stack.imgur.com/Ql0YP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c412b1",
   "metadata": {},
   "source": [
    "# Activacion\n",
    "\n",
    "## Nonlinear functions\n",
    "\n",
    "* (a) Sigmoid function\n",
    "* (b) Tanh function\n",
    "* (c) ReLU function\n",
    "* (d) Leaky ReLU function.\n",
    "\n",
    "> Se puede demostrar que sin estas funciones, cualquier red neuronal por mas compleja que sea se puede reducir a una red de una sola capa con coeficientes lineales. Volviendo a la vieja regresion logistica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3aa7ef",
   "metadata": {},
   "source": [
    "# Funciones de activacion\n",
    "\n",
    "![](https://www.researchgate.net/publication/323617663/figure/fig3/AS:667846732423172@1536238477220/Nonlinear-function-a-Sigmoid-function-b-Tanh-function-c-ReLU-function-d-Leaky.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92215c",
   "metadata": {},
   "source": [
    "# Redes? Capas? \n",
    "\n",
    "* En teoria una red neuronal de una sola capa con muchas neuronas, podria aproximar cualquier funcion, pero es mas eficiente entrenar una red de varias capaz de tamaño reducido.\n",
    "\n",
    "* Como controlamos la complejidad (capacidad de aprendizaje) de una red?\n",
    "\n",
    "    * Numero de Neuronas (width).\n",
    "    * Numero de Layers (depth).\n",
    "\n",
    "![](2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae601f1",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "...Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets\n",
    "with many hidden layers...\n",
    "``` \n",
    "                                        - Yoshua Bengio (2009), \"Learning Deep Architectures for AI\"\n",
    "\n",
    "* Deeper redes permiten usar menos parametros, las shallow redes no son buenas para abstraer conocimiento (reducir la dimensionalidad) https://arxiv.org/pdf/1312.6098.pdf\n",
    "\n",
    "![](2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1520443",
   "metadata": {},
   "source": [
    "* Relational and semantic knowledge can be obtained at higher levels of abstraction and representation of the raw data (Yoshua Bengio and Yann LeCun, Scaling Learning Algorithms towards AI, 2007).\n",
    "\n",
    "* Deep architectures can be representationally efficient. This sounds contradictory, but its a great benefit because of the distributed representation power by deep learning.\n",
    "\n",
    "* The learning capacity of deep learning algorithms is proportional to the size of data, that is, performance increases as the input data increases, whereas, for shallow or traditional learning algorithms, the performance reaches a plateau after a certain amount of data is provided as shown in the following figure, Learning capability of deep learning versus traditional machine learning:\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/0*GTzatEUd4cICPVub.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17c073",
   "metadata": {},
   "source": [
    "![](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec74a8",
   "metadata": {},
   "source": [
    "![](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086882f",
   "metadata": {},
   "source": [
    "![](5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cc74a",
   "metadata": {},
   "source": [
    "# Como entrena? Como encuentro los betas?\n",
    "\n",
    "Minimizo la funcion de costo, pero como encuentro los betas de tantas capas?\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "![](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22ebc6",
   "metadata": {},
   "source": [
    "# Optimizadores\n",
    "\n",
    "Veamos que pinta tiene una funcion de costo:\n",
    "\n",
    "![](6.png)\n",
    "\n",
    "### Adaptative Learning Rates\n",
    "\n",
    "### Batched Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050a531",
   "metadata": {},
   "source": [
    "# Regularizacion\n",
    "\n",
    "## Dropout\n",
    "\n",
    "De manera aleatoria matamos neuronas con una birrita en cada iteracion, obligamos a que todas las neuronas aprendan. Esto hace que la red sea robusta y que pueda generalizar. \n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Dejamos de iterar cuando en un set de cross validation empezamos a performar mal (una especie de cross validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b5a4d",
   "metadata": {},
   "source": [
    "## Cuando usar deep learning? tiramos todo lo que aprendimos en el curso?\n",
    "\n",
    "Miremos la performance de una NN en uno de los dataset que usamos en el curso, como el de 'hitter.csv' si, ese, malisimo! Ya se, pero bueno es uno de los que mas usamos (263 jugadores, 19 variables)\n",
    "\n",
    "Ajustamos esos datos con una regresion lineal, lasso y una NN. Aca los resultados:\n",
    "\n",
    "![](0.png)\n",
    "\n",
    "No solo no performo mejor que una regresion comun, si no que ademas tiene 1.4k parametros!\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9273f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659f4b2b",
   "metadata": {},
   "source": [
    "# Redes Multicapa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# activaciones\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "def relu_prime(x):\n",
    "    return np.array(x >= 0).astype('int')\n",
    "\n",
    "# errores\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "def sse(y_true, y_pred):\n",
    "    return 0.5 * np.sum(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_pred.size\n",
    "def sse_prime(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "\n",
    "\n",
    "# importamos los datos\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000]\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84cdc7",
   "metadata": {},
   "source": [
    "# Estructura de la Red\n",
    "\n",
    "```\n",
    "network = [\n",
    "    FlattenLayer(input_shape=(28, 28)),\n",
    "    FCLayer(28 * 28, 20),\n",
    "    ActivationLayer(relu, relu_prime),\n",
    "    FCLayer(20, 10),\n",
    "    SoftmaxLayer(10)\n",
    "]\n",
    "```\n",
    "\n",
    "Todas las capas tienen un metodo forward y backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshapea el input\n",
    "class FlattenLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, (1, -1))\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return np.reshape(output_error, self.input_shape)\n",
    "    \n",
    "# capa fully conected\n",
    "\n",
    "class FCLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)\n",
    "        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # bias_error = output_error\n",
    "        \n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error \n",
    "    \n",
    "# activacion (no linealidad)\n",
    "\n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        return output_error * self.activation_prime(self.input)\n",
    "    \n",
    "# reshapea el output, normaliza sobre la cantidad de clases\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        input_error = np.zeros(output_error.shape)\n",
    "        out = np.tile(self.output.T, self.input_size)\n",
    "        return self.output * np.dot(output_error, np.identity(self.input_size) - out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305c715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los datos\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000]\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b6b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    FlattenLayer(input_shape=(28, 28)),\n",
    "    FCLayer(28 * 28, 10),\n",
    "    SoftmaxLayer(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    FlattenLayer(input_shape=(28, 28)),\n",
    "    FCLayer(28 * 28, 10),\n",
    "    ActivationLayer(relu, relu_prime),\n",
    "    SoftmaxLayer(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    FlattenLayer(input_shape=(28, 28)),\n",
    "    FCLayer(28 * 28, 20),\n",
    "    ActivationLayer(relu, relu_prime),\n",
    "    FCLayer(20, 10),\n",
    "    SoftmaxLayer(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,x_train, y_train, epochs=40,learning_rate=0.1):\n",
    "    err=[]\n",
    "    # training\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for x, y_true in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = x\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            # error (display purpose only)\n",
    "            error += mse(y_true, output)\n",
    "\n",
    "            # backward\n",
    "            output_error = mse_prime(y_true, output)\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        err.append(error)\n",
    "    return(network,err)\n",
    "\n",
    "def predict(network, input):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dddbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network,err=train(network,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91136fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(err)\n",
    "plt.grid(True)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6cbb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9772b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos la performance en el set de testeo\n",
    "\n",
    "accuracy = sum([np.argmax(y) == np.argmax(predict(network, x)) for x, y in zip(x_test, y_test)]) / len(x_test)\n",
    "error = sum([mse(y, predict(network, x)) for x, y in zip(x_test, y_test)]) / len(x_test)\n",
    "print('accuracy: %.4f' % ratio)\n",
    "print('mse: %.4f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc122a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(3,4,figsize=(13,10))\n",
    "\n",
    "\n",
    "for i,ax in enumerate(axs.flat):\n",
    "    \n",
    "    image = np.reshape(test, (28, 28))\n",
    "    ax.imshow(image, cmap='binary')\n",
    "\n",
    "    pred = predict(network, test)[0]\n",
    "    idx = np.argmax(pred)\n",
    "    idx_true = np.argmax(true)\n",
    "    ax.text(0.5,2.5,'pred: %s, prob: %.3f, true: %d' % (idx, pred[idx], idx_true))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63f28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c99db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for test, true in zip(x_test[:samples], y_test[:samples]):\n",
    "    image = np.reshape(test, (28, 28))\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.show()\n",
    "    pred = predict(network, test)[0]\n",
    "    idx = np.argmax(pred)\n",
    "    idx_true = np.argmax(true)\n",
    "    print('pred: %s, prob: %.3f, true: %d' % (idx, pred[idx], idx_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b65b6",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "Libreria de google para redes neuronales, antiguamente se usaba una libreria aparte llamada KERAS para armar las redes, hoy keras esta incluida en TF.\n",
    "\n",
    "Esta es la arquitectura de nuestra red:\n",
    "\n",
    "```\n",
    "network = [\n",
    "    FlattenLayer(input_shape=(28, 28)),\n",
    "    FCLayer(28 * 28, 20),\n",
    "    ActivationLayer(relu, relu_prime),\n",
    "    FCLayer(20, 10),\n",
    "    SoftmaxLayer(10)\n",
    "]\n",
    "```\n",
    "\n",
    "Como implementamos esto en Tf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a8a9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ks.models.Sequential()\n",
    " \n",
    "model.add(ks.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(ks.layers.Dense(20,activation=tf.nn.relu))\n",
    "model.add(ks.layers.Dense(20,activation=tf.nn.relu))\n",
    "model.add(ks.layers.Dense(10,activation=tf.nn.softmax))\n",
    " \n",
    "model.compile(\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0ca76a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/32 [===============>..............] - ETA: 0s - loss: 2.1217 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 20:41:43.748377: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 6ms/step - loss: 2.0462 - accuracy: 0.2930\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.5990 - accuracy: 0.5640\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 1.2192 - accuracy: 0.7120\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.9180 - accuracy: 0.7790\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7284 - accuracy: 0.8290\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.5973 - accuracy: 0.8590\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.5071 - accuracy: 0.8800\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.4385 - accuracy: 0.9010\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.9130\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.3374 - accuracy: 0.9220\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32ef2404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37/313 [==>...........................] - ETA: 1s - loss: 0.5962 - accuracy: 0.8201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 20:41:47.935074: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5224 - accuracy: 0.8407\n",
      "loss->  0.5223953127861023 \n",
      "acc->  0.8407000303268433\n"
     ]
    }
   ],
   "source": [
    "val_loss,val_acc = model.evaluate(x_test,y_test)\n",
    "print(\"loss-> \",val_loss,\"\\nacc-> \",val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "914eb7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7042895e-04, 1.0681600e-05, 3.0386774e-03, 3.1250835e-04,\n",
       "       2.7212143e-04, 5.1969531e-05, 4.3267846e-06, 9.8679346e-01,\n",
       "       2.2903752e-05, 9.3228202e-03], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=model.predict([x_test])\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6875c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df2be319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ->  1\n",
      "prediction ->  1\n"
     ]
    }
   ],
   "source": [
    "print('label -> ',np.argmax(y_test[2]))\n",
    "print('prediction -> ',np.argmax(predictions[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3df74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ->  9\n",
      "prediction ->  9\n"
     ]
    }
   ],
   "source": [
    "print('label -> ',np.argmax(y_test[20]))\n",
    "print('prediction -> ',np.argmax(predictions[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5235c3",
   "metadata": {},
   "source": [
    "# nlp\n",
    "\n",
    "![](https://pythondiario.com/wp-content/uploads/2021/08/1_copilot.gif)\n",
    "\n",
    "\n",
    "\n",
    "http://nlpprogress.com/\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c37312",
   "metadata": {},
   "source": [
    "# audio\n",
    "\n",
    "https://openai.com/blog/jukebox/\n",
    "\n",
    "https://fakeyou.com\n",
    "https://fakeyou.com/tts/result/TR:aykxb3fa2vcgnr13mnyfy5ksjgcag\n",
    "\n",
    "Usando solo 5 segundos de entrenamiento!\n",
    "\n",
    "https://www.youtube.com/watch?v=0sR1rU3gLzQ\n",
    "\n",
    "https://app.resemble.ai/voices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6a594",
   "metadata": {},
   "source": [
    "# img\n",
    "\n",
    "## GANS \n",
    "\n",
    "https://thispersondoesnotexist.com\n",
    "\n",
    "\n",
    "https://openai.com/blog/image-gpt/\n",
    "https://openai.com/blog/dall-e/\n",
    "\n",
    "\n",
    "## deepfakes\n",
    "\n",
    "https://www.youtube.com/watch?v=HG_NZpkttXE\n",
    "\n",
    "\n",
    "## superesolucion\n",
    "\n",
    "https://www.youtube.com/watch?v=Fxd8XJ_J0Gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19257413",
   "metadata": {},
   "source": [
    "# img\n",
    "\n",
    "yolo (you only look once)\n",
    "\n",
    "![](https://pythonawesome.com/content/images/2021/07/track_all.gif)\n",
    "\n",
    "https://colab.research.google.com/github/aissam-out/YOLO/blob/master/YOLO_ImageAI_video.ipynb#scrollTo=tXvkvSC8eij8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7bb7d",
   "metadata": {},
   "source": [
    "# rl (reinforcement learning)\n",
    "\n",
    "https://www.youtube.com/watch?v=hx_bgoTF7bs\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=jjfDO2pWpys&t=378s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
